{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WineClassifierFinalYearProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPOmpQdQHyu2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxmN0UCoI79K"
      },
      "source": [
        "# WINE CLASSIFIER MODEL\n",
        "**The question?**\n",
        "\n",
        "We often \"google\" reviews while buying or testing various products. So here we are keeping in mind the sentiments of our user and making their life a bit easier by using a classifier to get the perfect review for Wine. \n",
        "Assume we have a collection of wine reviews. We'd want to anticipate if a review is in favour or not for each one. This is the same as stating we'd like to create a model that takes a review and predicts whether a particular wine will be good or bad. (To put it another way, we'd like to create a model that takes a document d and predicts a class.)\n",
        "\n",
        "**The Approach**\n",
        "\n",
        "We use Naive Bayes algorithm to classify the Wine. \n",
        "\n",
        "**Hypothesis**\n",
        "\n",
        "This algorithm should prove to be fairly accurate, however, one would not expect to have one hundred percent accuracy.\n",
        "\n",
        "**What is Machine Learning**\n",
        "\n",
        "Machine learning (ML) is a sort of artificial intelligence (AI) that allows software applications to improve their prediction accuracy without being expressly designed to do so. In order to forecast new output values, machine learning algorithms use historical data as input.\n",
        "\n",
        "Machine Learning depends on three things, Classification, Prediction and Regression.\n",
        "\n",
        "Classification : The algorithm's job is to produce a recipe that separates the data if we want to automate a classification process. \n",
        "What we have here is a data set with two classifications labeled on it (Y and N). To put it another way, the algorithm must create a fence in the data to best separate the Y’s and N’s. Our goal is to develop a model that can be used to appropriately classify fresh cases. What shape will be fitted to the data is determined by the algorithm.The information is utilized to determine where the optimum location for the fence is.\n",
        "Prediction: If classification is the process of categorizing data into groups, prediction is the process of fitting a form to the data that is as close to the data as possible. The object we're fitting resembles a skeleton that runs through a single body of data rather than a fence that separates two bodies of data. \n",
        "As before, the algorithm provides the WHAT, while the data provides the WHERE. New data points are plugged into the formula, and the anticipated value is read from the line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f120bU2gs43l"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilf_MPY0sPFt",
        "outputId": "7053a1c9-913b-4e3c-dbb3-81f6b99d9598"
      },
      "source": [
        "from csv import reader\n",
        "import random\n",
        "\n",
        "def load_csv(filename):\n",
        "    dataset = list()\n",
        "    with open(filename, 'r') as file:\n",
        "        csv_reader = reader(file)\n",
        "        for row in csv_reader:\n",
        "            if not row:\n",
        "                continue\n",
        "            dataset.append(row)\n",
        "    return dataset\n",
        "\n",
        "df = load_csv('/content/wine-dataset.data')\n",
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "178"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSSOTWzwsVAI",
        "outputId": "82097f3c-5624-4841-aaa5-d2d47c63c74a"
      },
      "source": [
        "# Converting strings to floats\n",
        "for i in range(len(df)):\n",
        "    for j in range(len(df[i])):\n",
        "        df[i][j]=float(df[i][j])\n",
        "#Checking if converted right\n",
        "type(df[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9X_asvExDvj"
      },
      "source": [
        "train, test = df[:145], df[146:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wYa7sO8wBzX"
      },
      "source": [
        "# len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-BJ8O4iv69P"
      },
      "source": [
        "# idx = random.randrange(len(df))\n",
        "# idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QNupPxZsibv"
      },
      "source": [
        "# def split_data(data, weight):\n",
        "#     train_length = int(len(data) * weight)\n",
        "#     train = []\n",
        "#     for i in range(train_length):\n",
        "#         idx = random.randrange(len(data))\n",
        "#         train.append(data[idx])\n",
        "#         data.pop(idx)\n",
        "#     return [train, data]\n",
        "\n",
        "# train, test = split_data(df, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBlldGZsnWZ"
      },
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "for i in range(len(train)):\n",
        "    y_train.append(train[i][0])\n",
        "    X_train.append(train[i][1:])\n",
        "    \n",
        "for i in range(len(test)):\n",
        "    y_test.append(test[i][0])\n",
        "    X_test.append(test[i][1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8KdCsmXu7zZ",
        "outputId": "a01954fc-5dd4-48fd-c87a-358c56141e65"
      },
      "source": [
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(145, 145, 32, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WikRl90Idv5"
      },
      "source": [
        "Naive Bayes Algorithm \n",
        "\n",
        "It's a classification method based on Bayes' Theorem and the assumption of predictor independence. A Naive Bayes classifier, in simple terms, posits that the existence of one feature in a class is unrelated to the presence of any other feature.\n",
        "\n",
        "Steps to use using Naive Bayes classifier: \n",
        "\n",
        "1. Load required libraries- NumPy is the sole library you'll need to create your own Naive Bayes classifier. NumPy is an open source project that aims to make numerical computation possible with Python, and we'll be using it to do arithmetic operations. \n",
        "\n",
        "2. Instantiate the class- The next step is to create a new instance of our Naive Bayes classifier. A class functions similarly to an object constructor or a \"blueprint\" for constructing things. Almost everything in an object-oriented programming language is an object, complete with properties and methods. \n",
        "\n",
        "3. Separate Classes- The Bayes Theorem states that we must first determine the prior probability of any class before attempting to predict it. To do so, we must first assign the feature values to the appropriate class. This can be accomplished by splitting the classes and storing them in a dictionary. \n",
        "Dictionaries are Python's implementation of an associative array, which is more often known as a data structure. A dictionary is made up of a set of key-value pairs. Each key-value pair corresponds to a certain value.\n",
        "\n",
        "4. Summary of Features- The likelihood, or the probability of a predictor given a class, is computed using mean and standard deviation and is considered to be normally distributed (Gaussian) (see formula). We'll produce a summary for each feature in the data set, which will make it easier to retrieve the mean and standard deviation of features in the future. \n",
        "\n",
        "5. The Gaussian distribution function is used to calculate the likelihood of features that follow a normal distribution.\n",
        "\n",
        "6. Train the model- Training the model entails applying it to a dataset so that it can iterate through it and learn the dataset's patterns. The mean and standard deviation for each feature of each class are calculated in the Naive Bayes classifier during training. This will enable us to determine the probabilities that will be utilized to make forecasts. \n",
        "\n",
        "7. Predict- In order to predict a class, we must first calculate its posterior probability. The predicted class will be the one with the highest posterior probability. \n",
        "The posterior probability is calculated by dividing the joint probability by the marginal probability. The denominator, or marginal probability, is the total joint probability of all classes, and it will be the same for all classes. The class with the highest posterior probability, also known as the greatest joint probability, is required.\n",
        "Predict the class- Once we have the joint probability for each class, we may choose the class with the highest joint probability. \n",
        "Putting it all together- We can forecast the class for each row in a test data set by combining the joint probability and predict class steps.\n",
        "\n",
        "8. Calculating the accuracy score is an important aspect of any machine learning model's testing. To evaluate the performance of our Naive Bayes classifier, we divide the number of right predictions by the total number of predictions, yielding a number ranging from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hDXnZlvspt1"
      },
      "source": [
        "class NaiveBayesClassifier:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def separate_classes(self, X, y):\n",
        "        \"\"\"\n",
        "        Separates the dataset in to a subset of data for each class.\n",
        "        Parameters:\n",
        "        ------------\n",
        "        X- array, list of features\n",
        "        y- list, target\n",
        "        Returns:\n",
        "        A dictionnary with y as keys and assigned X as values.\n",
        "        \"\"\"\n",
        "        separated_classes = {}\n",
        "        for i in range(len(X)):\n",
        "            feature_values = X[i]\n",
        "            class_name = y[i]\n",
        "            if class_name not in separated_classes:\n",
        "                separated_classes[class_name] = []\n",
        "            separated_classes[class_name].append(feature_values)\n",
        "        return separated_classes\n",
        "\n",
        "    def stat_info(self, X):\n",
        "        \"\"\"\n",
        "        Calculates standard deviation and mean of features.\n",
        "        Parameters:\n",
        "        ------------\n",
        "        X- array , list of features\n",
        "        Returns:\n",
        "        A dictionary with STD and Mean as keys and assigned features STD and Mean as values.\n",
        "        \"\"\"\n",
        "        for feature in zip(*X):\n",
        "            yield {\n",
        "                'std' : np.std(feature),\n",
        "                'mean' : np.mean(feature)\n",
        "            }\n",
        "    \n",
        "    def fit (self, X, y):\n",
        "        \"\"\"\n",
        "        Trains the model.\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X: array-like, training features\n",
        "        y: list, target variable\n",
        "        Returns:\n",
        "        Dictionary with the prior probability, mean, and standard deviation of each class\n",
        "        \"\"\"\n",
        "\n",
        "        separated_classes = self.separate_classes(X, y)\n",
        "        self.class_summary = {}\n",
        "\n",
        "        for class_name, feature_values in separated_classes.items():\n",
        "            self.class_summary[class_name] = {\n",
        "                'prior_proba': len(feature_values)/len(X),\n",
        "                'summary': [i for i in self.stat_info(feature_values)],\n",
        "            }\n",
        "        return self.class_summary\n",
        "\n",
        "    def distribution(self, x, mean, std):\n",
        "        \"\"\"\n",
        "        Gaussian Distribution Function\n",
        "        Parameters:\n",
        "        ----------\n",
        "        x: float, value of feature\n",
        "        mean: float, the average value of feature\n",
        "        stdev: float, the standard deviation of feature\n",
        "        Returns:\n",
        "        A value of Normal Probability\n",
        "        \"\"\"\n",
        "\n",
        "        exponent = np.exp(-((x-mean)**2 / (2*std**2)))\n",
        "\n",
        "        return exponent / (np.sqrt(2*np.pi)*std)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predicts the class.\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X: array-like, test data set\n",
        "        Returns:\n",
        "        -----------\n",
        "        List of predicted class for each row of data set\n",
        "        \"\"\"\n",
        "        MAPs = []\n",
        "\n",
        "        for row in X:\n",
        "            joint_proba = {}\n",
        "            \n",
        "            for class_name, features in self.class_summary.items():\n",
        "                total_features =  len(features['summary'])\n",
        "                likelihood = 1\n",
        "\n",
        "                for idx in range(total_features):\n",
        "                    feature = row[idx]\n",
        "                    mean = features['summary'][idx]['mean']\n",
        "                    stdev = features['summary'][idx]['std']\n",
        "                    normal_proba = self.distribution(feature, mean, stdev)\n",
        "                    likelihood *= normal_proba\n",
        "                prior_proba = features['prior_proba']\n",
        "                joint_proba[class_name] = prior_proba * likelihood\n",
        "\n",
        "            MAP = max(joint_proba, key= joint_proba.get)\n",
        "            MAPs.append(MAP)\n",
        "\n",
        "        return MAPs\n",
        "\n",
        "    def accuracy(self, y_test, y_pred):\n",
        "        \"\"\"\n",
        "        Calculates model's accuracy.\n",
        "        Parameters:\n",
        "        ------------\n",
        "        y_test: actual values\n",
        "        y_pred: predicted values\n",
        "        Returns:\n",
        "        ------------\n",
        "        A number between 0-1, representing the percentage of correct predictions.\n",
        "        \"\"\"\n",
        "\n",
        "        true_true = 0.0\n",
        "\n",
        "        for y_t, y_p in zip(y_test, y_pred):\n",
        "            if y_t == y_p:\n",
        "                true_true += 1 \n",
        "        print(len(y_test), true_true)\n",
        "        return true_true / len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QycAi1DasyWx",
        "outputId": "94cb5ddd-8539-47c1-cb55-18b5e025d71c"
      },
      "source": [
        "model = NaiveBayesClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "acc = model.accuracy(y_test, y_pred)\n",
        "print (\"NaiveBayesClassifier accuracy: {0:.6f}\".format(model.accuracy(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 30.0\n",
            "32 30.0\n",
            "NaiveBayesClassifier accuracy: 0.937500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYxv8ajbs19r",
        "outputId": "004bb4cb-ad00-401f-8927-54f9f6139f81"
      },
      "source": [
        "type(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pj-GmhxtPVa",
        "outputId": "d67a7b92-2790-4928-f682-58c06a406e4a"
      },
      "source": [
        "for p, a in zip(y_pred, y_test):\n",
        "  if (p != a):\n",
        "    print(p, a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0 3.0\n",
            "1.0 3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTMxMTyJtVt7",
        "outputId": "9b8b6c6c-08d5-4c7b-fb74-c5366c7f8eb0"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "sk_model = GaussianNB()\n",
        "sk_model.fit(X_train, y_train)\n",
        "y_pred = sk_model.predict(X_test)\n",
        "print(\"Scikit-learn GaussianNB accuracy: {0:.6f}\".format(accuracy_score(y_test, y_pred)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn GaussianNB accuracy: 0.937500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFVaVgv_HuvN"
      },
      "source": [
        "**Conclusion** \n",
        "\n",
        "This pretty much sums up the entire project and the work displayed through the Classifier System. Challenges faced during the project was mainly the coding part and finding the right dataset that would be an appropriate match for our problem statement. \n",
        "The project was very insightful and on detailed study, the entire group unanimously agreed that the Classifying system is one of the most expensive technology in the foreseeable future, and we decided on working on it, making note worthy changes that would turn our study into a potential research someday.\n",
        "\n",
        " Hoping for the best! "
      ]
    }
  ]
}